[
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Linear and Non Linear regression",
    "section": "",
    "text": "Blog 2 – Linear and Non Linear regression\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nImport packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nLoad the data\n\ndata = pd.read_csv(\"housing.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThe info function provide the information about the dataset . For example:\n\nMissing values(no missing values in our dataset)\ndatatype(9 of them are floats and 1 is categorical)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nPearson correlation :\n\nplt.subplots(figsize=(15, 9))\ndata_numeric = data.select_dtypes(include=['float64', 'int64'])\ncor = data_numeric.corr()\nsns.heatmap(cor, annot=True, linewidths=.5)\nplt.show()\n\n\n\n\nIf we have to select a single variable for the regression analysis then higher possibility is to pick the most correlated feature with the target variable(median_house_value).\n\nIn our case it is the median_income with correlation coefficent of 0.69\n\n\n# taking two variables\ndata = data.drop([\"housing_median_age\",\"households\",\"total_bedrooms\",\"longitude\",\"latitude\",\"total_rooms\",\"population\",\"ocean_proximity\"], axis=1)\ndata.head()\n\n\n\n\n\n\n\n\nmedian_income\nmedian_house_value\n\n\n\n\n0\n8.3252\n452600.0\n\n\n1\n8.3014\n358500.0\n\n\n2\n7.2574\n352100.0\n\n\n3\n5.6431\n341300.0\n\n\n4\n3.8462\n342200.0\n\n\n\n\n\n\n\nUsing this scatter plot we can infer that if a person has higher median_income then that person may have more expensive house. There is somewhat positive linear relationship between them.\n\nX = data.drop(\"median_house_value\", axis=1)\ny = data[\"median_house_value\"]\nplt.scatter(X, y, alpha=0.5)\nplt.title('Scatter plot')\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\nplt.show()\n\n\n\n\n\nSplit the data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\n\nModel 1:\n\nLinear regression model\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Model initialization\nregression_model = LinearRegression()\n\n# Fit the data(train the model)\nregression_model.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Predict\ny_predicted = regression_model.predict(X_test)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [42032.17769894]\nIntercept: 44320.6352276571\nRoot mean squared error:  84941.05152406936\nR2 score:  0.4466846804895944\n\n\nInterpretation:\nThis simple linear regression with single variable (y = mx+b) has\n\nSlope of the line(m) : [42032.17769894]\nIntercept (b) : 44320.63\nR2 score: 0.4466 (For R2 score more is better in the range [0,1])\nRoot mean squared error: 84941.0515 (Lower is better)\n\n#####The plot of simple linear regression :\n\n# data points\nplt.scatter(X_train, y_train, s=10)\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\n\n# predicted values\nplt.plot(X_test, y_predicted, color='r')\nplt.show()\n\n\n\n\n\n\n\n\nModel 2:\nFitting polynomial Regression model\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ndef viz_polymonial():\n    plt.scatter(X_train, y_train, color=\"red\")\n    plt.plot(X_train, pol_reg.predict(poly_reg.fit_transform(X_train)))\n    plt.xlabel('median_income')\n    plt.ylabel('median_house_value')\n    plt.show()\n    return\nviz_polymonial()\n\n\n\n\n\n# Predict\nX_p = poly_reg.fit_transform(X_test)\ny_predicted = pol_reg.predict(X_p)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [42032.17769894]\nIntercept: 44320.6352276571\nRoot mean squared error:  84699.90676455045\nR2 score:  0.44982190770645947\n\n\nInterpretation:\nThis transformed linear regression with single variable (y = mx+b) has\n\nSlope of the line(m) : 175550.81\nIntercept (b) : -129097.46\nR2 score: 0.4498 (For R2 score more is better in the range [0,1])\nFound R2 score is the best so far. This means that we will keep this ploynomial model with degree 2 as our final and best model(but there is one other thing to consider i.e. simple is better than complex)\n\nRoot mean squared error: 84699.9 (Lower is better)\n\n\nComparing the Model\n\nModel 1 has R2 score: 0.4466\nModel 2 has R2 score: 0.44982 After analyzing the R2 score , My final model will be Model 1 as it is simple and has not worse R2 score as compared to the model 3."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "K Means Clustering",
    "section": "",
    "text": "Blog 1 – Clustering\nThe aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc.\nThis is a post with executable code.\nWe will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\nNext, let’s read the data and look at the first five rows:\n\ndata=pd.read_csv(\"customer_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nChannel\nRegion\nFresh\nMilk\nGrocery\nFrozen\nDetergents_Paper\nDelicassen\n\n\n\n\n0\n2\n3\n12669\n9656\n7561\n214\n2674\n1338\n\n\n1\n2\n3\n7057\n9810\n9568\n1762\n3293\n1776\n\n\n2\n2\n3\n6353\n8808\n7684\n2405\n3516\n7844\n\n\n3\n1\n3\n13265\n1196\n4221\n6404\n507\n1788\n\n\n4\n2\n3\n22615\n5410\n7198\n3915\n1777\n5185\n\n\n\n\n\n\n\nHere, we see that there is a lot of variation in the magnitude of the data. Variables like Channel and Region have low magnitude, whereas variables like Fresh, Milk, Grocery, etc., have a higher magnitude.\nSince K-Means is a distance-based algorithm, this difference in magnitude can create a problem. Bring all the variables to the same magnitude\n\n# standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# statistics of scaled data\npd.DataFrame(data_scaled).describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\ncount\n4.400000e+02\n4.400000e+02\n4.400000e+02\n440.000000\n4.400000e+02\n4.400000e+02\n4.400000e+02\n4.400000e+02\n\n\nmean\n1.614870e-17\n3.552714e-16\n-3.431598e-17\n0.000000\n-4.037175e-17\n3.633457e-17\n2.422305e-17\n-8.074349e-18\n\n\nstd\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138e+00\n\n\nmin\n-6.902971e-01\n-1.995342e+00\n-9.496831e-01\n-0.778795\n-8.373344e-01\n-6.283430e-01\n-6.044165e-01\n-5.402644e-01\n\n\n25%\n-6.902971e-01\n-7.023369e-01\n-7.023339e-01\n-0.578306\n-6.108364e-01\n-4.804306e-01\n-5.511349e-01\n-3.964005e-01\n\n\n50%\n-6.902971e-01\n5.906683e-01\n-2.767602e-01\n-0.294258\n-3.366684e-01\n-3.188045e-01\n-4.336004e-01\n-1.985766e-01\n\n\n75%\n1.448652e+00\n5.906683e-01\n3.905226e-01\n0.189092\n2.849105e-01\n9.946441e-02\n2.184822e-01\n1.048598e-01\n\n\nmax\n1.448652e+00\n5.906683e-01\n7.927738e+00\n9.183650\n8.936528e+00\n1.191900e+01\n7.967672e+00\n1.647845e+01\n\n\n\n\n\n\n\nCreate a kmeans function and fit it on the data\n\n# defining the kmeans function with initialization as k-means++\nkmeans = KMeans(n_clusters=2, init='k-means++')\n\n# fitting the k means algorithm on scaled data\nkmeans.fit(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nKMeans(n_clusters=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=2)\n\n\nLet’s evaluate how well the formed clusters are. To do that, we will calculate the inertia of the clusters:\n\n# inertia on the fitted data\nkmeans.inertia_\n\n2599.38555935614\n\n\nWe will store the inertia value of each model and then plot it to visualize the result\n\nSSE = []\nfor cluster in range(1,20):\n    kmeans = KMeans(n_clusters = cluster, init='k-means++')\n    kmeans.fit(data_scaled)\n    SSE.append(kmeans.inertia_)\n\n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nText(0, 0.5, 'Inertia')\n\n\n\n\n\nLooking at the above elbow curve, we can choose any number of clusters between 5 to 8.\nSet the number of clusters as 6 and fit the model.\n\n# k means using 5 clusters and k-means++ initialization\nkmeans = KMeans(n_clusters = 5, init='k-means++')\nkmeans.fit(data_scaled)\npred = kmeans.predict(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nValue count of points in each of the above-formed clusters\n\nframe = pd.DataFrame(data_scaled)\nframe['cluster'] = pred\nframe['cluster'].value_counts()\n\ncluster\n0    211\n3    126\n2     92\n1     10\n4      1\nName: count, dtype: int64\n\n\nSo, there are 234 data points belonging to cluster 4 (index 3), 125 points in cluster 2 (index 1), and so on.\nNow, lets plot the clusters according to midpoints and differentiate based on colours.\n\nplt.scatter(data_scaled[:, 2], data_scaled[:, 3], c=pred, s=50, cmap='viridis')\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 2], centers[:, 4], c='black', s=200, alpha=0.5);"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myBlog",
    "section": "",
    "text": "Random Variables\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non Linear regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nK Means Clustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/random-variables/index.html",
    "href": "posts/random-variables/index.html",
    "title": "Random Variables",
    "section": "",
    "text": "This is a post with executable code.\nWe will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\nimport time\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\n\ndef pseudo_uniform_bad(mult=5,\n                       mod=11,\n                       seed=1,\n                       size=1):\n    \"\"\"\n    A bad pseudo random generator with small multipliers and modulus\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_bad(seed=3,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\ndef pseudo_uniform_good(mult=16807,\n                        mod=(2**31)-1,\n                        seed=123456789,\n                        size=1):\n    \"\"\"\n    A reasoanbly good pseudo random generator\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_good(size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef sample_pick(lst):\n    \"\"\"\n    Picks up a random sample from a given list\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    # Random sample as an index\n    l = len(lst)\n    s = pseudo_uniform(low=0,high=l,seed=seed,size=1)\n    idx = int(s)\n    \n    return (lst[idx])\n\ndef pseudo_uniform(low=0,\n                   high=1,\n                  seed=123456789,\n                  size=1):\n    \"\"\"\n    Generates uniformly random number between `low` and `high` limits\n    \"\"\"\n    return low+(high-low)*pseudo_uniform_good(seed=seed,size=size)\n\ndice_faces = ['one','two','three','four','five','six']\n\n\nfor _ in range(30):\n    print(sample_pick(dice_faces),end=', ')\n\nsix, three, five, six, one, one, two, three, three, four, four, five, five, six, six, one, two, two, three, three, four, four, five, five, six, one, one, two, two, three, \n\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1321908839.py:11: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  idx = int(s)\n\n\n\nl = []\nfor _ in range(10000):\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    l.append(float(pseudo_uniform(0,6,seed=seed,size=1)))\n    \nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlim(-1,7)\nplt.show()\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\3436541096.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  l.append(float(pseudo_uniform(0,6,seed=seed,size=1)))\n\n\n\n\n\n\n\n\n\ndef pseudo_bernoulli(p=0.5,size=1):\n    \"\"\"\n    Bernoulli generator from uniform generator\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    B = pseudo_uniform(seed=seed,size=size)\n    B = (B&lt;=p).astype(int)\n    \n    return B\n\nl=pseudo_bernoulli(p=0.2,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_binomial(n=100,\n                   p=0.5,\n                   size=1):\n    \"\"\"\n    Binomial distribution from the Uniform generator\n    \"\"\"\n    binom = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(size=n,seed=seed)\n        Y = (U &lt;= p).astype(int)\n        binom.append(np.sum(Y))\n    \n    return binom\n\n\n# 100 loaded coins, each with probability of head 0.75, are flipped \n# This trial/experiment is repeated for 15 times\n# The number of heads in each experiment are given below\npseudo_binomial(n=100,p=0.75,size=15)\n\n[75, 71, 85, 70, 70, 74, 76, 77, 81, 78, 70, 77, 75, 77, 79]\n\n\n\n\n\n\ndef pseudo_normal(mu=0.0,sigma=1.0,size=1):\n    \"\"\"\n    Generates Normal distribution from the Uniform distribution using Box-Muller transform\n    \"\"\"\n    # A pair of Uniform distributions\n    t = time.perf_counter()\n    seed1 = int(10**9*float(str(t-int(t))[0:]))\n    U1 = pseudo_uniform(seed=seed1,size=size)\n    t = time.perf_counter()\n    seed2 = int(10**9*float(str(t-int(t))[0:]))\n    U2 = pseudo_uniform(seed=seed2,size=size)\n    # Standard Normal pair\n    Z0 = np.sqrt(-2*np.log(U1))*np.cos(2*np.pi*U2)\n    Z1 = np.sqrt(-2*np.log(U1))*np.sin(2*np.pi*U2)\n    # Scaling\n    Z0 = Z0*sigma+mu\n    \n    return Z0\n\nl1=pseudo_normal(size=10000)\nplt.hist(l1,bins=25,edgecolor='k',alpha=0.5,color='blue')\nl2=pseudo_normal(mu=-3,sigma=2.0,size=10000)\nplt.hist(l2,bins=25,edgecolor='k',alpha=0.5,color='red')\nl3=pseudo_normal(mu=3,sigma=0.5,size=10000)\nplt.hist(l3,bins=25,edgecolor='k',alpha=0.5,color='green')\n\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend([\"$\\mu$:0, $\\sigma$:1.0\",\n           \"$\\mu$:-3, $\\sigma$:2.0\",\n           \"$\\mu$:3, $\\sigma$:0.5\"],fontsize=14)\nplt.show()\n\n&lt;&gt;:29: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:30: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:29: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:30: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1355573055.py:29: SyntaxWarning: invalid escape sequence '\\m'\n  plt.legend([\"$\\mu$:0, $\\sigma$:1.0\",\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1355573055.py:30: SyntaxWarning: invalid escape sequence '\\m'\n  \"$\\mu$:-3, $\\sigma$:2.0\",\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1355573055.py:31: SyntaxWarning: invalid escape sequence '\\m'\n  \"$\\mu$:3, $\\sigma$:0.5\"],fontsize=14)\n\n\n\n\n\n\nl=pseudo_normal(size=10000)\nplt.hist(l,bins=25,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_exp(lamb,size=1):\n    \"\"\"\n    Generates exponential distribution from the Uniform distribution\n    \"\"\"\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    U = pseudo_uniform(size=size,seed=seed)\n    X = -(1/lamb)*(np.log(1-U))\n    \n    return X\n\nl=pseudo_exp(lamb=0.1,size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_poisson(alpha,size=1):\n    \"\"\"\n    \"\"\"\n    poisson = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(seed=seed,size=5*alpha)\n        X,P,i = 0,1,0\n        while P &gt;= np.exp(-alpha):\n            P = U[i]*P\n            X+=1\n            i+=1\n        poisson.append(X)\n    return np.array(poisson)\n\nl1=pseudo_poisson(alpha=5,size=10000)\nl2=pseudo_poisson(alpha=10,size=10000)\nl3=pseudo_poisson(alpha=20,size=10000)\n\nd1=dict(Counter(l1))\nd2=dict(Counter(l2))\nd3=dict(Counter(l3))\n\nk1 = [k for k in d1.keys()]\nv1 = [v for v in d1.values()]\nk2 = [k for k in d2.keys()]\nv2 = [v for v in d2.values()]\nk3 = [k for k in d3.keys()]\nv3 = [v for v in d3.values()]\n\n# Plotting\nplt.scatter(k1,v1,c='blue')\nplt.scatter(k2,v2,c='k')\nplt.scatter(k3,v3,c='green')\nplt.legend([\"Rate parameter \"+\"$\\lambda$: \"+str(i) for i in (5,10,20)],fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n&lt;&gt;:36: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:36: SyntaxWarning: invalid escape sequence '\\l'\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\2970150594.py:36: SyntaxWarning: invalid escape sequence '\\l'\n  plt.legend([\"Rate parameter \"+\"$\\lambda$: \"+str(i) for i in (5,10,20)],fontsize=15)"
  },
  {
    "objectID": "posts/random-variables/index.html#generating-random-variables-from-scratch",
    "href": "posts/random-variables/index.html#generating-random-variables-from-scratch",
    "title": "Random Variables",
    "section": "",
    "text": "This is a post with executable code.\nWe will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\nimport time\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\n\ndef pseudo_uniform_bad(mult=5,\n                       mod=11,\n                       seed=1,\n                       size=1):\n    \"\"\"\n    A bad pseudo random generator with small multipliers and modulus\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_bad(seed=3,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\ndef pseudo_uniform_good(mult=16807,\n                        mod=(2**31)-1,\n                        seed=123456789,\n                        size=1):\n    \"\"\"\n    A reasoanbly good pseudo random generator\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_good(size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef sample_pick(lst):\n    \"\"\"\n    Picks up a random sample from a given list\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    # Random sample as an index\n    l = len(lst)\n    s = pseudo_uniform(low=0,high=l,seed=seed,size=1)\n    idx = int(s)\n    \n    return (lst[idx])\n\ndef pseudo_uniform(low=0,\n                   high=1,\n                  seed=123456789,\n                  size=1):\n    \"\"\"\n    Generates uniformly random number between `low` and `high` limits\n    \"\"\"\n    return low+(high-low)*pseudo_uniform_good(seed=seed,size=size)\n\ndice_faces = ['one','two','three','four','five','six']\n\n\nfor _ in range(30):\n    print(sample_pick(dice_faces),end=', ')\n\nsix, three, five, six, one, one, two, three, three, four, four, five, five, six, six, one, two, two, three, three, four, four, five, five, six, one, one, two, two, three, \n\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1321908839.py:11: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  idx = int(s)\n\n\n\nl = []\nfor _ in range(10000):\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    l.append(float(pseudo_uniform(0,6,seed=seed,size=1)))\n    \nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlim(-1,7)\nplt.show()\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\3436541096.py:5: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  l.append(float(pseudo_uniform(0,6,seed=seed,size=1)))\n\n\n\n\n\n\n\n\n\ndef pseudo_bernoulli(p=0.5,size=1):\n    \"\"\"\n    Bernoulli generator from uniform generator\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    B = pseudo_uniform(seed=seed,size=size)\n    B = (B&lt;=p).astype(int)\n    \n    return B\n\nl=pseudo_bernoulli(p=0.2,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_binomial(n=100,\n                   p=0.5,\n                   size=1):\n    \"\"\"\n    Binomial distribution from the Uniform generator\n    \"\"\"\n    binom = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(size=n,seed=seed)\n        Y = (U &lt;= p).astype(int)\n        binom.append(np.sum(Y))\n    \n    return binom\n\n\n# 100 loaded coins, each with probability of head 0.75, are flipped \n# This trial/experiment is repeated for 15 times\n# The number of heads in each experiment are given below\npseudo_binomial(n=100,p=0.75,size=15)\n\n[75, 71, 85, 70, 70, 74, 76, 77, 81, 78, 70, 77, 75, 77, 79]\n\n\n\n\n\n\ndef pseudo_normal(mu=0.0,sigma=1.0,size=1):\n    \"\"\"\n    Generates Normal distribution from the Uniform distribution using Box-Muller transform\n    \"\"\"\n    # A pair of Uniform distributions\n    t = time.perf_counter()\n    seed1 = int(10**9*float(str(t-int(t))[0:]))\n    U1 = pseudo_uniform(seed=seed1,size=size)\n    t = time.perf_counter()\n    seed2 = int(10**9*float(str(t-int(t))[0:]))\n    U2 = pseudo_uniform(seed=seed2,size=size)\n    # Standard Normal pair\n    Z0 = np.sqrt(-2*np.log(U1))*np.cos(2*np.pi*U2)\n    Z1 = np.sqrt(-2*np.log(U1))*np.sin(2*np.pi*U2)\n    # Scaling\n    Z0 = Z0*sigma+mu\n    \n    return Z0\n\nl1=pseudo_normal(size=10000)\nplt.hist(l1,bins=25,edgecolor='k',alpha=0.5,color='blue')\nl2=pseudo_normal(mu=-3,sigma=2.0,size=10000)\nplt.hist(l2,bins=25,edgecolor='k',alpha=0.5,color='red')\nl3=pseudo_normal(mu=3,sigma=0.5,size=10000)\nplt.hist(l3,bins=25,edgecolor='k',alpha=0.5,color='green')\n\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend([\"$\\mu$:0, $\\sigma$:1.0\",\n           \"$\\mu$:-3, $\\sigma$:2.0\",\n           \"$\\mu$:3, $\\sigma$:0.5\"],fontsize=14)\nplt.show()\n\n&lt;&gt;:29: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:30: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:29: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:30: SyntaxWarning: invalid escape sequence '\\m'\n&lt;&gt;:31: SyntaxWarning: invalid escape sequence '\\m'\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1355573055.py:29: SyntaxWarning: invalid escape sequence '\\m'\n  plt.legend([\"$\\mu$:0, $\\sigma$:1.0\",\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1355573055.py:30: SyntaxWarning: invalid escape sequence '\\m'\n  \"$\\mu$:-3, $\\sigma$:2.0\",\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\1355573055.py:31: SyntaxWarning: invalid escape sequence '\\m'\n  \"$\\mu$:3, $\\sigma$:0.5\"],fontsize=14)\n\n\n\n\n\n\nl=pseudo_normal(size=10000)\nplt.hist(l,bins=25,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_exp(lamb,size=1):\n    \"\"\"\n    Generates exponential distribution from the Uniform distribution\n    \"\"\"\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    U = pseudo_uniform(size=size,seed=seed)\n    X = -(1/lamb)*(np.log(1-U))\n    \n    return X\n\nl=pseudo_exp(lamb=0.1,size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_poisson(alpha,size=1):\n    \"\"\"\n    \"\"\"\n    poisson = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(seed=seed,size=5*alpha)\n        X,P,i = 0,1,0\n        while P &gt;= np.exp(-alpha):\n            P = U[i]*P\n            X+=1\n            i+=1\n        poisson.append(X)\n    return np.array(poisson)\n\nl1=pseudo_poisson(alpha=5,size=10000)\nl2=pseudo_poisson(alpha=10,size=10000)\nl3=pseudo_poisson(alpha=20,size=10000)\n\nd1=dict(Counter(l1))\nd2=dict(Counter(l2))\nd3=dict(Counter(l3))\n\nk1 = [k for k in d1.keys()]\nv1 = [v for v in d1.values()]\nk2 = [k for k in d2.keys()]\nv2 = [v for v in d2.values()]\nk3 = [k for k in d3.keys()]\nv3 = [v for v in d3.values()]\n\n# Plotting\nplt.scatter(k1,v1,c='blue')\nplt.scatter(k2,v2,c='k')\nplt.scatter(k3,v3,c='green')\nplt.legend([\"Rate parameter \"+\"$\\lambda$: \"+str(i) for i in (5,10,20)],fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n&lt;&gt;:36: SyntaxWarning: invalid escape sequence '\\l'\n&lt;&gt;:36: SyntaxWarning: invalid escape sequence '\\l'\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_3728\\2970150594.py:36: SyntaxWarning: invalid escape sequence '\\l'\n  plt.legend([\"Rate parameter \"+\"$\\lambda$: \"+str(i) for i in (5,10,20)],fontsize=15)"
  }
]