[
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Linear and Non Linear regression",
    "section": "",
    "text": "Blog 2 - Linear and Non Linear regression\nImport packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nLoad the data\n\ndata = pd.read_csv(\"housing.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThe info function provide the information about the dataset . For example:\n\nMissing values(no missing values in our dataset)\ndatatype(9 of them are floats and 1 is categorical)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nPearson correlation :\n\nplt.subplots(figsize=(15, 9))\ndata_numeric = data.select_dtypes(include=['float64', 'int64'])\ncor = data_numeric.corr()\nsns.heatmap(cor, annot=True, linewidths=.5)\nplt.show()\n\n\n\n\nIf we have to select a single variable for the regression analysis then higher possibility is to pick the most correlated feature with the target variable(median_house_value).\n\nIn our case it is the median_income with correlation coefficent of 0.69\n\n\n# taking two variables\ndata = data.drop([\"housing_median_age\",\"households\",\"total_bedrooms\",\"longitude\",\"latitude\",\"total_rooms\",\"population\",\"ocean_proximity\"], axis=1)\ndata.head()\n\n\n\n\n\n\n\n\nmedian_income\nmedian_house_value\n\n\n\n\n0\n8.3252\n452600.0\n\n\n1\n8.3014\n358500.0\n\n\n2\n7.2574\n352100.0\n\n\n3\n5.6431\n341300.0\n\n\n4\n3.8462\n342200.0\n\n\n\n\n\n\n\nUsing this scatter plot we can infer that if a person has higher median_income then that person may have more expensive house. There is somewhat positive linear relationship between them.\n\nX = data.drop(\"median_house_value\", axis=1)\ny = data[\"median_house_value\"]\nplt.scatter(X, y, alpha=0.5)\nplt.title('Scatter plot')\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\nplt.show()\n\n\n\n\n\nSplit the data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\n\nModel 1:\n\nLinear regression model\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Model initialization\nregression_model = LinearRegression()\n\n# Fit the data(train the model)\nregression_model.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Predict\ny_predicted = regression_model.predict(X_test)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [42032.17769894]\nIntercept: 44320.6352276571\nRoot mean squared error:  84941.05152406936\nR2 score:  0.4466846804895944\n\n\nInterpretation:\nThis simple linear regression with single variable (y = mx+b) has\n\nSlope of the line(m) : [42032.17769894]\nIntercept (b) : 44320.63\nR2 score: 0.4466 (For R2 score more is better in the range [0,1])\nRoot mean squared error: 84941.0515 (Lower is better)\n\n#####The plot of simple linear regression :\n\n# data points\nplt.scatter(X_train, y_train, s=10)\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\n\n# predicted values\nplt.plot(X_test, y_predicted, color='r')\nplt.show()\n\n\n\n\n\n\n\n\nModel 2:\nFitting polynomial Regression model\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ndef viz_polymonial():\n    plt.scatter(X_train, y_train, color=\"red\")\n    plt.plot(X_train, pol_reg.predict(poly_reg.fit_transform(X_train)))\n    plt.xlabel('median_income')\n    plt.ylabel('median_house_value')\n    plt.show()\n    return\nviz_polymonial()\n\n\n\n\n\n# Predict\nX_p = poly_reg.fit_transform(X_test)\ny_predicted = pol_reg.predict(X_p)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [42032.17769894]\nIntercept: 44320.6352276571\nRoot mean squared error:  84699.90676455045\nR2 score:  0.44982190770645947\n\n\nInterpretation:\nThis transformed linear regression with single variable (y = mx+b) has\n\nSlope of the line(m) : 175550.81\nIntercept (b) : -129097.46\nR2 score: 0.4498 (For R2 score more is better in the range [0,1])\nFound R2 score is the best so far. This means that we will keep this ploynomial model with degree 2 as our final and best model(but there is one other thing to consider i.e. simple is better than complex)\n\nRoot mean squared error: 84699.9 (Lower is better)\n\n\nComparing the Model\n\nModel 1 has R2 score: 0.4466\nModel 2 has R2 score: 0.44982 After analyzing the R2 score , My final model will be Model 1 as it is simple and has not worse R2 score as compared to the model 3."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Segment the Clients of a Wholesale Distributor",
    "section": "",
    "text": "Blog 1 – Clustering\nThe aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc.\nThis is a post with executable code.\nWe will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\nNext, let’s read the data and look at the first five rows:\n\ndata=pd.read_csv(\"customer_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nChannel\nRegion\nFresh\nMilk\nGrocery\nFrozen\nDetergents_Paper\nDelicassen\n\n\n\n\n0\n2\n3\n12669\n9656\n7561\n214\n2674\n1338\n\n\n1\n2\n3\n7057\n9810\n9568\n1762\n3293\n1776\n\n\n2\n2\n3\n6353\n8808\n7684\n2405\n3516\n7844\n\n\n3\n1\n3\n13265\n1196\n4221\n6404\n507\n1788\n\n\n4\n2\n3\n22615\n5410\n7198\n3915\n1777\n5185\n\n\n\n\n\n\n\nHere, we see that there is a lot of variation in the magnitude of the data. Variables like Channel and Region have low magnitude, whereas variables like Fresh, Milk, Grocery, etc., have a higher magnitude.\nSince K-Means is a distance-based algorithm, this difference in magnitude can create a problem. Bring all the variables to the same magnitude\n\n# standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# statistics of scaled data\npd.DataFrame(data_scaled).describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\ncount\n4.400000e+02\n4.400000e+02\n4.400000e+02\n440.000000\n4.400000e+02\n4.400000e+02\n4.400000e+02\n4.400000e+02\n\n\nmean\n1.614870e-17\n3.552714e-16\n-3.431598e-17\n0.000000\n-4.037175e-17\n3.633457e-17\n2.422305e-17\n-8.074349e-18\n\n\nstd\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138e+00\n\n\nmin\n-6.902971e-01\n-1.995342e+00\n-9.496831e-01\n-0.778795\n-8.373344e-01\n-6.283430e-01\n-6.044165e-01\n-5.402644e-01\n\n\n25%\n-6.902971e-01\n-7.023369e-01\n-7.023339e-01\n-0.578306\n-6.108364e-01\n-4.804306e-01\n-5.511349e-01\n-3.964005e-01\n\n\n50%\n-6.902971e-01\n5.906683e-01\n-2.767602e-01\n-0.294258\n-3.366684e-01\n-3.188045e-01\n-4.336004e-01\n-1.985766e-01\n\n\n75%\n1.448652e+00\n5.906683e-01\n3.905226e-01\n0.189092\n2.849105e-01\n9.946441e-02\n2.184822e-01\n1.048598e-01\n\n\nmax\n1.448652e+00\n5.906683e-01\n7.927738e+00\n9.183650\n8.936528e+00\n1.191900e+01\n7.967672e+00\n1.647845e+01\n\n\n\n\n\n\n\nCreate a kmeans function and fit it on the data\n\n# defining the kmeans function with initialization as k-means++\nkmeans = KMeans(n_clusters=2, init='k-means++')\n\n# fitting the k means algorithm on scaled data\nkmeans.fit(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nKMeans(n_clusters=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=2)\n\n\nLet’s evaluate how well the formed clusters are. To do that, we will calculate the inertia of the clusters:\n\n# inertia on the fitted data\nkmeans.inertia_\n\n2599.38555935614\n\n\nWe will store the inertia value of each model and then plot it to visualize the result\n\nSSE = []\nfor cluster in range(1,20):\n    kmeans = KMeans(n_clusters = cluster, init='k-means++')\n    kmeans.fit(data_scaled)\n    SSE.append(kmeans.inertia_)\n\n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nText(0, 0.5, 'Inertia')\n\n\n\n\n\nLooking at the above elbow curve, we can choose any number of clusters between 5 to 8.\nSet the number of clusters as 6 and fit the model.\n\n# k means using 5 clusters and k-means++ initialization\nkmeans = KMeans(n_clusters = 5, init='k-means++')\nkmeans.fit(data_scaled)\npred = kmeans.predict(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\nValue count of points in each of the above-formed clusters\n\nframe = pd.DataFrame(data_scaled)\nframe['cluster'] = pred\nframe['cluster'].value_counts()\n\ncluster\n0    200\n1    126\n4     90\n3     14\n2     10\nName: count, dtype: int64\n\n\nSo, there are 234 data points belonging to cluster 4 (index 3), 125 points in cluster 2 (index 1), and so on.\nNow, lets plot the clusters according to midpoints and differentiate based on colours.\n\nplt.scatter(data_scaled[:, 2], data_scaled[:, 3], c=pred, s=50, cmap='viridis')\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 2], centers[:, 4], c='black', s=200, alpha=0.5);"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "Implementation of Isolation Forest to Detect Outliers in Python (Scikit-learn)\n\n\n\n\n\n\n\nAnomaly/outlier detection\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nImplementation of Isolation Forest to Detect Outliers in Python (Scikit-learn)\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nGenerating Random Variables from Scratch\n\n\n\n\n\n\n\nRandom Variables\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Non Linear regression\n\n\n\n\n\n\n\nLinear and Non Linear regression\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nSegment the Clients of a Wholesale Distributor\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/random-variables/index.html",
    "href": "posts/random-variables/index.html",
    "title": "Generating Random Variables from Scratch",
    "section": "",
    "text": "We will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\nimport time\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\n\ndef pseudo_uniform_bad(mult=5,\n                       mod=11,\n                       seed=1,\n                       size=1):\n    \"\"\"\n    A bad pseudo random generator with small multipliers and modulus\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_bad(seed=3,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\ndef pseudo_uniform_good(mult=16807,\n                        mod=(2**31)-1,\n                        seed=123456789,\n                        size=1):\n    \"\"\"\n    A reasoanbly good pseudo random generator\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_good(size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef sample_pick(lst):\n    \"\"\"\n    Picks up a random sample from a given list\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    # Random sample as an index\n    l = len(lst)\n    s = pseudo_uniform(low=0,high=l,seed=seed,size=1)\n    idx = int(s)\n    \n    return (lst[idx])\n\ndef pseudo_uniform(low=0,\n                   high=1,\n                  seed=123456789,\n                  size=1):\n    \"\"\"\n    Generates uniformly random number between `low` and `high` limits\n    \"\"\"\n    return low+(high-low)*pseudo_uniform_good(seed=seed,size=size)\n\ndice_faces = ['one','two','three','four','five','six']\n\n\nfor _ in range(30):\n    print(sample_pick(dice_faces),end=', ')\n\none, one, two, three, three, four, four, five, five, five, six, six, one, one, two, two, two, three, three, four, four, five, five, five, six, six, one, one, two, two, \n\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1321908839.py:11: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n\n\n\n\nl = []\nfor _ in range(10000):\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    l.append(float(pseudo_uniform(0,6,seed=seed,size=1)))\n    \nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlim(-1,7)\nplt.show()\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\3436541096.py:5: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n\n\n\n\n\n\n\n\n\n\ndef pseudo_bernoulli(p=0.5,size=1):\n    \"\"\"\n    Bernoulli generator from uniform generator\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    B = pseudo_uniform(seed=seed,size=size)\n    B = (B&lt;=p).astype(int)\n    \n    return B\n\nl=pseudo_bernoulli(p=0.2,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_binomial(n=100,\n                   p=0.5,\n                   size=1):\n    \"\"\"\n    Binomial distribution from the Uniform generator\n    \"\"\"\n    binom = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(size=n,seed=seed)\n        Y = (U &lt;= p).astype(int)\n        binom.append(np.sum(Y))\n    \n    return binom\n\n\n# 100 loaded coins, each with probability of head 0.75, are flipped \n# This trial/experiment is repeated for 15 times\n# The number of heads in each experiment are given below\npseudo_binomial(n=100,p=0.75,size=15)\n\n[67, 75, 67, 76, 74, 72, 75, 73, 81, 74, 80, 75, 75, 77, 76]\n\n\n\n\n\n\ndef pseudo_normal(mu=0.0,sigma=1.0,size=1):\n    \"\"\"\n    Generates Normal distribution from the Uniform distribution using Box-Muller transform\n    \"\"\"\n    # A pair of Uniform distributions\n    t = time.perf_counter()\n    seed1 = int(10**9*float(str(t-int(t))[0:]))\n    U1 = pseudo_uniform(seed=seed1,size=size)\n    t = time.perf_counter()\n    seed2 = int(10**9*float(str(t-int(t))[0:]))\n    U2 = pseudo_uniform(seed=seed2,size=size)\n    # Standard Normal pair\n    Z0 = np.sqrt(-2*np.log(U1))*np.cos(2*np.pi*U2)\n    Z1 = np.sqrt(-2*np.log(U1))*np.sin(2*np.pi*U2)\n    # Scaling\n    Z0 = Z0*sigma+mu\n    \n    return Z0\n\nl1=pseudo_normal(size=10000)\nplt.hist(l1,bins=25,edgecolor='k',alpha=0.5,color='blue')\nl2=pseudo_normal(mu=-3,sigma=2.0,size=10000)\nplt.hist(l2,bins=25,edgecolor='k',alpha=0.5,color='red')\nl3=pseudo_normal(mu=3,sigma=0.5,size=10000)\nplt.hist(l3,bins=25,edgecolor='k',alpha=0.5,color='green')\n\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend([\"$\\mu$:0, $\\sigma$:1.0\",\n           \"$\\mu$:-3, $\\sigma$:2.0\",\n           \"$\\mu$:3, $\\sigma$:0.5\"],fontsize=14)\nplt.show()\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1355573055.py:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1355573055.py:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1355573055.py:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n\n\n\n\n\n\nl=pseudo_normal(size=10000)\nplt.hist(l,bins=25,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_exp(lamb,size=1):\n    \"\"\"\n    Generates exponential distribution from the Uniform distribution\n    \"\"\"\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    U = pseudo_uniform(size=size,seed=seed)\n    X = -(1/lamb)*(np.log(1-U))\n    \n    return X\n\nl=pseudo_exp(lamb=0.1,size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_poisson(alpha,size=1):\n    \"\"\"\n    \"\"\"\n    poisson = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(seed=seed,size=5*alpha)\n        X,P,i = 0,1,0\n        while P &gt;= np.exp(-alpha):\n            P = U[i]*P\n            X+=1\n            i+=1\n        poisson.append(X)\n    return np.array(poisson)\n\nl1=pseudo_poisson(alpha=5,size=10000)\nl2=pseudo_poisson(alpha=10,size=10000)\nl3=pseudo_poisson(alpha=20,size=10000)\n\nd1=dict(Counter(l1))\nd2=dict(Counter(l2))\nd3=dict(Counter(l3))\n\nk1 = [k for k in d1.keys()]\nv1 = [v for v in d1.values()]\nk2 = [k for k in d2.keys()]\nv2 = [v for v in d2.values()]\nk3 = [k for k in d3.keys()]\nv3 = [v for v in d3.values()]\n\n# Plotting\nplt.scatter(k1,v1,c='blue')\nplt.scatter(k2,v2,c='k')\nplt.scatter(k3,v3,c='green')\nplt.legend([\"Rate parameter \"+\"$\\lambda$: \"+str(i) for i in (5,10,20)],fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n&lt;&gt;:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'\n\n&lt;&gt;:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\2970150594.py:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'"
  },
  {
    "objectID": "posts/random-variables/index.html#generating-random-variables-from-scratch",
    "href": "posts/random-variables/index.html#generating-random-variables-from-scratch",
    "title": "Generating Random Variables from Scratch",
    "section": "",
    "text": "We will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\nimport time\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\n\ndef pseudo_uniform_bad(mult=5,\n                       mod=11,\n                       seed=1,\n                       size=1):\n    \"\"\"\n    A bad pseudo random generator with small multipliers and modulus\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_bad(seed=3,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\ndef pseudo_uniform_good(mult=16807,\n                        mod=(2**31)-1,\n                        seed=123456789,\n                        size=1):\n    \"\"\"\n    A reasoanbly good pseudo random generator\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_good(size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef sample_pick(lst):\n    \"\"\"\n    Picks up a random sample from a given list\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    # Random sample as an index\n    l = len(lst)\n    s = pseudo_uniform(low=0,high=l,seed=seed,size=1)\n    idx = int(s)\n    \n    return (lst[idx])\n\ndef pseudo_uniform(low=0,\n                   high=1,\n                  seed=123456789,\n                  size=1):\n    \"\"\"\n    Generates uniformly random number between `low` and `high` limits\n    \"\"\"\n    return low+(high-low)*pseudo_uniform_good(seed=seed,size=size)\n\ndice_faces = ['one','two','three','four','five','six']\n\n\nfor _ in range(30):\n    print(sample_pick(dice_faces),end=', ')\n\none, one, two, three, three, four, four, five, five, five, six, six, one, one, two, two, two, three, three, four, four, five, five, five, six, six, one, one, two, two, \n\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1321908839.py:11: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n\n\n\n\nl = []\nfor _ in range(10000):\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    l.append(float(pseudo_uniform(0,6,seed=seed,size=1)))\n    \nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlim(-1,7)\nplt.show()\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\3436541096.py:5: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n\n\n\n\n\n\n\n\n\n\ndef pseudo_bernoulli(p=0.5,size=1):\n    \"\"\"\n    Bernoulli generator from uniform generator\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    B = pseudo_uniform(seed=seed,size=size)\n    B = (B&lt;=p).astype(int)\n    \n    return B\n\nl=pseudo_bernoulli(p=0.2,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_binomial(n=100,\n                   p=0.5,\n                   size=1):\n    \"\"\"\n    Binomial distribution from the Uniform generator\n    \"\"\"\n    binom = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(size=n,seed=seed)\n        Y = (U &lt;= p).astype(int)\n        binom.append(np.sum(Y))\n    \n    return binom\n\n\n# 100 loaded coins, each with probability of head 0.75, are flipped \n# This trial/experiment is repeated for 15 times\n# The number of heads in each experiment are given below\npseudo_binomial(n=100,p=0.75,size=15)\n\n[67, 75, 67, 76, 74, 72, 75, 73, 81, 74, 80, 75, 75, 77, 76]\n\n\n\n\n\n\ndef pseudo_normal(mu=0.0,sigma=1.0,size=1):\n    \"\"\"\n    Generates Normal distribution from the Uniform distribution using Box-Muller transform\n    \"\"\"\n    # A pair of Uniform distributions\n    t = time.perf_counter()\n    seed1 = int(10**9*float(str(t-int(t))[0:]))\n    U1 = pseudo_uniform(seed=seed1,size=size)\n    t = time.perf_counter()\n    seed2 = int(10**9*float(str(t-int(t))[0:]))\n    U2 = pseudo_uniform(seed=seed2,size=size)\n    # Standard Normal pair\n    Z0 = np.sqrt(-2*np.log(U1))*np.cos(2*np.pi*U2)\n    Z1 = np.sqrt(-2*np.log(U1))*np.sin(2*np.pi*U2)\n    # Scaling\n    Z0 = Z0*sigma+mu\n    \n    return Z0\n\nl1=pseudo_normal(size=10000)\nplt.hist(l1,bins=25,edgecolor='k',alpha=0.5,color='blue')\nl2=pseudo_normal(mu=-3,sigma=2.0,size=10000)\nplt.hist(l2,bins=25,edgecolor='k',alpha=0.5,color='red')\nl3=pseudo_normal(mu=3,sigma=0.5,size=10000)\nplt.hist(l3,bins=25,edgecolor='k',alpha=0.5,color='green')\n\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend([\"$\\mu$:0, $\\sigma$:1.0\",\n           \"$\\mu$:-3, $\\sigma$:2.0\",\n           \"$\\mu$:3, $\\sigma$:0.5\"],fontsize=14)\nplt.show()\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1355573055.py:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1355573055.py:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\1355573055.py:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n\n\n\n\n\n\nl=pseudo_normal(size=10000)\nplt.hist(l,bins=25,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_exp(lamb,size=1):\n    \"\"\"\n    Generates exponential distribution from the Uniform distribution\n    \"\"\"\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    U = pseudo_uniform(size=size,seed=seed)\n    X = -(1/lamb)*(np.log(1-U))\n    \n    return X\n\nl=pseudo_exp(lamb=0.1,size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_poisson(alpha,size=1):\n    \"\"\"\n    \"\"\"\n    poisson = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(seed=seed,size=5*alpha)\n        X,P,i = 0,1,0\n        while P &gt;= np.exp(-alpha):\n            P = U[i]*P\n            X+=1\n            i+=1\n        poisson.append(X)\n    return np.array(poisson)\n\nl1=pseudo_poisson(alpha=5,size=10000)\nl2=pseudo_poisson(alpha=10,size=10000)\nl3=pseudo_poisson(alpha=20,size=10000)\n\nd1=dict(Counter(l1))\nd2=dict(Counter(l2))\nd3=dict(Counter(l3))\n\nk1 = [k for k in d1.keys()]\nv1 = [v for v in d1.values()]\nk2 = [k for k in d2.keys()]\nv2 = [v for v in d2.values()]\nk3 = [k for k in d3.keys()]\nv3 = [v for v in d3.values()]\n\n# Plotting\nplt.scatter(k1,v1,c='blue')\nplt.scatter(k2,v2,c='k')\nplt.scatter(k3,v3,c='green')\nplt.legend([\"Rate parameter \"+\"$\\lambda$: \"+str(i) for i in (5,10,20)],fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n&lt;&gt;:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'\n\n&lt;&gt;:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_36600\\2970150594.py:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Blog 3 – Classification\n\nImplementation of Isolation Forest to Detect Outliers in Python (Scikit-learn)\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n\nDiabetes and Pima Indian Dataset\n\nMachine learning has become an important approach to many researches today. It is a field of artificial intelligence and its importance is increasing day by day.\nIn this project, a classification model will be made using the Pima Indians Diabetes data set. Diabetes is a group of metabolic disorders in which long-term high blood sugar levels are seen. High blood sugar symptoms include frequent urination, increased thirst, and increased hunger. If left untreated, diabetes can cause many complications. Acute complications may include diabetic ketoacidosis, hyperosmolar hyperglycemic state, or death. Serious long-term complications include cardiovascular disease, stroke, chronic kidney disease, foot ulcer, and eye damage.\nThis dataset was originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The purpose of the dataset is to diagnostically predict whether a patient has diabetes based on the specific diagnostic measures included in the data set. Various restrictions have been imposed on the selection of these samples from a larger database.\nIn particular, all the patients here are women who are at least 21 years old of Pima Indian heritage. The data set consists of 768 observation units and 9 variables. These variables are; pregnancy, glucose, blood pressure, skin thickness, insulin, body mass index, diabetes pedigree, age and outcome. After the exploratory data analysis on the data set is completed, the machine learning model will be set up. For this, supervised learning algorithms will be used.\n\n\n\nImporting Libraries\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV, RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score, recall_score, f1_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\nReading Data\n\ndiabetes = pd.read_csv(\"diabetes.csv\")\ndf = diabetes.copy()\ndf.head()\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n\n\n\n\n\n\ndf.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nPregnancies\n768.0\n3.845052\n3.369578\n0.000\n1.00000\n3.0000\n6.00000\n17.00\n\n\nGlucose\n768.0\n120.894531\n31.972618\n0.000\n99.00000\n117.0000\n140.25000\n199.00\n\n\nBloodPressure\n768.0\n69.105469\n19.355807\n0.000\n62.00000\n72.0000\n80.00000\n122.00\n\n\nSkinThickness\n768.0\n20.536458\n15.952218\n0.000\n0.00000\n23.0000\n32.00000\n99.00\n\n\nInsulin\n768.0\n79.799479\n115.244002\n0.000\n0.00000\n30.5000\n127.25000\n846.00\n\n\nBMI\n768.0\n31.992578\n7.884160\n0.000\n27.30000\n32.0000\n36.60000\n67.10\n\n\nDiabetesPedigreeFunction\n768.0\n0.471876\n0.331329\n0.078\n0.24375\n0.3725\n0.62625\n2.42\n\n\nAge\n768.0\n33.240885\n11.760232\n21.000\n24.00000\n29.0000\n41.00000\n81.00\n\n\nOutcome\n768.0\n0.348958\n0.476951\n0.000\n0.00000\n0.0000\n1.00000\n1.00\n\n\n\n\n\n\n\nIn this dataset missing data are filled with 0. First, we are gonna change zeros with NaN\n\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness',\n                                                                      'Insulin','BMI']].replace(0, np.NaN)\n\n\n\nData Visualization\n\nHistogram\nA histogram is a bar graph representation of a grouped data distribution. In other words, it is the transfer of data consisting of repetitive numbers to the table first, and to the chart by using the table, in other words, the graph of the data groups is displayed in rectangular columns.\n\ndf.hist(bins=20,figsize = (15,15));\n\n\n\n\n\n\nCountplot and PiePlot\nA count plot can be thought of as a histogram across a categorical, instead of quantitative, variable. A Pie Chart is a type of graph that displays data in a circular graph. The pieces of the graph are proportional to the fraction of the whole in each category.\nWe examined distribution of outcome with countplot and pieplot.\n\nplt.title(\"Distribution of Outcome\")\nsns.countplot(df[\"Outcome\"], saturation=1)\n\n&lt;Axes: title={'center': 'Distribution of Outcome'}, ylabel='count'&gt;\n\n\n\n\n\n\ndef PlotPie(df, nameOfFeature):\n    labels = [str(df[nameOfFeature].unique()[i]) for i in range(df[nameOfFeature].nunique())]\n    values = [df[nameOfFeature].value_counts()[i] for i in range(df[nameOfFeature].nunique())]\n\n    trace=go.Pie(labels=labels,values=values)\n\n    py.iplot([trace])\n\nPlotPie(df, \"Outcome\")\n\n\n                                                \n\n\n\n\nCorrelation\nCorrelation is a term that is a measure of the strength of a linear relationship between two quantitative variables.\nIn this graph, there are correlations of all variables with the Outcome variable.\n\ndef corr_to_target(dataframe, target, title=None, file=None):\n    plt.figure(figsize=(4,6))\n    sns.heatmap(dataframe.corr()[[target]].sort_values(target,\n                                                        ascending=False)[1:],\n                                                        annot=True,\n                                                        cmap='coolwarm')\n    \n    plt.title(f'\\n{title}\\n', fontsize=18)\n    \n    plt.show();\n    \n    return\n\ncorr_to_target(df, \"Outcome\", title=\"Outcome\")\n\n\n\n\nCorrelation matrix of variables with each other.\n\ncorr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot=True, fmt=\".2f\")\nplt.title(\"Correlation Between Features\")\n\nText(0.5, 1.0, 'Correlation Between Features')\n\n\n\n\n\n\n\n\nSplitting Train and Test Set\nAbove, we first gave all variables except the “outcome” variable to the X variable and gave the variable “outcome” to the y variable. Then we split the data into train and test data. X_train and y_train show the dependent and independent variables to be used to test the model, while X_test and y_test are used to develop the model. Test_size specifies how many% of data (30%) will be used for testing. Random_state is used to see the same distinction every time we run the program. Stratify provides a balanced separation of classes in the y variable when separating.\n\n#y = df[\"Outcome\"]\n#X = df.drop([\"Outcome\"], axis = 1)\ntrain,test = train_test_split(df, test_size=0.3, random_state = 2)\n\n\ntrain.isnull().sum()\n\nPregnancies                   0\nGlucose                       2\nBloodPressure                22\nSkinThickness               162\nInsulin                     256\nBMI                           7\nDiabetesPedigreeFunction      0\nAge                           0\nOutcome                       0\ndtype: int64\n\n\n\ntest.isnull().sum()\n\nPregnancies                   0\nGlucose                       3\nBloodPressure                13\nSkinThickness                65\nInsulin                     118\nBMI                           4\nDiabetesPedigreeFunction      0\nAge                           0\nOutcome                       0\ndtype: int64\n\n\n\n\nHandling with Missing Values\nAfter filling the 0s with the value of NaN, the missing values ​​will be visualized. We use the missingno library for this.\n\nmsno.bar(df,figsize=(10,6))\n\n&lt;Axes: &gt;\n\n\n\n\n\nWe will fill in each missing value with its median value.\n\ndef median_target(dataf, var):   \n    temp = dataf[dataf[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp\n\n\ncolumns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']\nfor i in columns:\n    train.loc[(train['Outcome'] == 0 ) & (train[i].isnull()), i] = median_target(train,i)[i][0]\n    train.loc[(train['Outcome'] == 1 ) & (train[i].isnull()), i] = median_target(train,i)[i][1]\n    \n    test.loc[(test['Outcome'] == 0 ) & (test[i].isnull()), i] = median_target(train,i)[i][0]\n    test.loc[(test['Outcome'] == 1 ) & (test[i].isnull()), i] = median_target(train,i)[i][1]\n\nAfter filling if we examine null values in dataset, we will see there are not any missing values.\n\nprint(\"TRAIN DATA\")\nprint(train.isnull().sum(), \"\\n\")\nprint(\"TEST DATA\")\nprint(test.isnull().sum())\n\nTRAIN DATA\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64 \n\nTEST DATA\nPregnancies                 0\nGlucose                     0\nBloodPressure               0\nSkinThickness               0\nInsulin                     0\nBMI                         0\nDiabetesPedigreeFunction    0\nAge                         0\nOutcome                     0\ndtype: int64\n\n\n\n\nPlotting Roc Curve\nROC curves are frequently used to show in a graphical way the connection/trade-off between clinical sensitivity and specificity for every possible cut-off for a test or a combination of tests. In addition the area under the ROC curve gives an idea about the benefit of using the test(s) in question.\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1],[0,1],\"k--\")\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n\n\n\nMachine Learning\nWe will use 6 different machine learning algorithm for this model and examine ROC score, accuracy test and train score, best parameters and ROC curve\n\nX_train = train.iloc[:,:8]\ny_train = train.iloc[:,-1:]\n\nX_test = test.iloc[:,:8]\ny_test = test.iloc[:,-1:]\n\n\ndef ml_model(model, parameters):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\n    random_search = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1, verbose=1)\n    pipe = make_pipeline(StandardScaler(),random_search)\n    pipe.fit(X_train, y_train)\n    y_pred_proba = pipe.predict_proba(X_test)[:,1]\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n    print(\"ROC Score : \",roc_auc_score(y_test, y_pred_proba))\n    print(\"F1 score for train: \", f1_score(y_train, pipe.predict(X_train)))\n    print(\"F1 score for test: \" , f1_score(y_test, pipe.predict(X_test)))\n    print(\"Best params:\" + str(random_search.best_params_))\n    plot_roc_curve(fpr, tpr)\n    \nlog_reg_params = {\"C\" : [1,2,3,0.01,0.001, 2.5, 1.5],\n                  \"max_iter\" : range(100,800,100)}\nknn_params = {\"n_neighbors\" : np.arange(1,50),\n              \"leaf_size\" : np.arange(1,50)}\ndecTree_params = {\"max_depth\" : [5,10,15,20,25,30],\n                  \"min_samples_split\" : np.arange(2,50),\n                  \"min_samples_leaf\" : np.arange(1,50)}\nrandomForest_params = {\"n_estimators\" : [100,500, 1000],\n                       \"min_samples_split\" : np.arange(2,30),\n                       \"min_samples_leaf\" : np.arange(1,50),\n                       \"max_features\" : np.arange(1,7)}\nlgbm_params = {\"n_estimators\" : [100,500,1000],\n               \"subsample\" : [0.6,0.8,1.0],\n               \"max_depth\" : [5,10,15,20,25,30],\n               \"learning_rate\" : [0.1, 0.01, 0.02, 0.5],\n               \"min_child_samples\" : np.arange(2,30)}\n\nsgd_params = {\"alpha\" : [0.0001, 0.1, 0.001, 0.01],\n              \"max_iter\" : [100,500,1000,2000],\n              \"loss\" : [\"log\",\"modified_huber\",\"perceptron\"]}\n\n\nml_model(LogisticRegression(), log_reg_params)\n\nFitting 100 folds for each of 10 candidates, totalling 1000 fits\nROC Score :  0.8506791171477079\nF1 score for train:  0.6523076923076924\nF1 score for test:  0.5573770491803278\nBest params:{'max_iter': 700, 'C': 0.01}\n\n\n\n\n\n\nml_model(KNeighborsClassifier(), knn_params)\n\nFitting 100 folds for each of 10 candidates, totalling 1000 fits\nROC Score :  0.8732173174872666\nF1 score for train:  0.7647058823529411\nF1 score for test:  0.7007299270072993\nBest params:{'n_neighbors': 19, 'leaf_size': 23}\n\n\n\n\n\n\nml_model(DecisionTreeClassifier(), decTree_params)\n\nFitting 100 folds for each of 10 candidates, totalling 1000 fits\nROC Score :  0.9217741935483872\nF1 score for train:  0.8385542168674699\nF1 score for test:  0.8104575163398693\nBest params:{'min_samples_split': 13, 'min_samples_leaf': 33, 'max_depth': 25}\n\n\n\n\n\n\nml_model(RandomForestClassifier(), randomForest_params)\n\nFitting 100 folds for each of 10 candidates, totalling 1000 fits\nROC Score :  0.9337011884550086\nF1 score for train:  0.8733850129198966\nF1 score for test:  0.7857142857142857\nBest params:{'n_estimators': 1000, 'min_samples_split': 24, 'min_samples_leaf': 13, 'max_features': 2}\n\n\n\n\n\n\nml_model(LGBMClassifier(), lgbm_params)\n\nFitting 100 folds for each of 10 candidates, totalling 1000 fits\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 192, number of negative: 345\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000159 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 620\n[LightGBM] [Info] Number of data points in the train set: 537, number of used features: 8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.357542 -&gt; initscore=-0.586049\n[LightGBM] [Info] Start training from score -0.586049\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\nROC Score :  0.9449066213921902\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\nF1 score for train:  0.907103825136612\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth &gt; num_leaves. (num_leaves=31).\nF1 score for test:  0.7851851851851851\nBest params:{'subsample': 1.0, 'n_estimators': 100, 'min_child_samples': 24, 'max_depth': 20, 'learning_rate': 0.02}\n\n\n\n\n\n\nml_model(SGDClassifier(), sgd_params)\n\nFitting 100 folds for each of 10 candidates, totalling 1000 fits\nROC Score :  0.775\nF1 score for train:  0.5993690851735015\nF1 score for test:  0.5210084033613445\nBest params:{'max_iter': 2000, 'loss': 'modified_huber', 'alpha': 0.001}"
  },
  {
    "objectID": "posts/anomaly/index.html",
    "href": "posts/anomaly/index.html",
    "title": "Implementation of Isolation Forest to Detect Outliers in Python (Scikit-learn)",
    "section": "",
    "text": "Blog 5 – Anomaly/outlier detection\n\n1. Preparation\n\n1.1. Input data\nThe data “sample_detect_outliers.csv” contains US public firms’ features that are related to leases.\n\n\n1.2. Output data\nThe following lines of code will output an indicator variable that equals 1 if the firm (e.g., observation) is an outlier and 0 otherwise. For more details on the steps after we identify outliers.\n\n\n1.3. Feature (i.e., variable) definition\n\nlag_lease: prior year’s lease activity\nlag_market_value: prior year’s market capitalization of the stock\nlag_dividend: an indicator value that equals 1 if the company paid any dividends in the prior, and 0 otherwise\nlag_loss: an indicator value that equals 1 if the company reported negative profits in the prior year\nlag_cash: prior year’s cash balance\nlag_tax_rate: effective tax rate in the prior year\nlag_big4_auditor: an indicator value that equals 1 if the company hired a Big 4 auditor in the prior year\n\n\n\n1.4. Import libraries and the data set\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\n# Import the data set\nsample_with_outliers = pd.read_csv('sample_detect_outliers.csv')\n\n\n\n\n2. Conduct an Exploratory Data Analysis (EDA)\n\n2.1. Show the first 5 entries in the data\n\n# First 5 entries\nsample_with_outliers.head()\n\n\n\n\n\n\n\n\nidentifier\nlease\nlag_lease\nlag_market_value\nlag_dividend\nlag_loss\nlag_cash\nlag_tax_rate\nlag_big4_auditor\n\n\n\n\n0\n1004\n1.716172\n1.300330\n6.953107\n0.007855\n0\n0.036106\n0.362751\n1\n\n\n1\n1004\n1.664240\n1.802926\n6.736387\n0.007212\n0\n0.021635\n0.359671\n1\n\n\n2\n1004\n1.163486\n1.595639\n7.090351\n0.006781\n0\n0.006848\n0.354971\n1\n\n\n3\n1004\n1.416672\n1.147767\n7.346950\n0.006755\n0\n0.027284\n0.210000\n1\n\n\n4\n1050\n0.626734\n0.659966\n5.562565\n0.013321\n1\n0.065985\n0.350000\n0\n\n\n\n\n\n\n\n\n\n2.2. The # of entries and the # of features\nThere are 6,279 entries and 9 variables = 1 identifier + 1 label (i.e., outcome variable) + 7 firm features. The second variable, lease, is the label.\n\n# (Number of entries, Number of features)\nprint(sample_with_outliers.shape)\n\n(6279, 9)\n\n\n\n\n2.3. Empirical distributions and histograms\nThere are many interesting observations that are worth noting. First, there are no missing values as indicated by counts that all equal 6,279.\n\n# Show empirical distributions\nsample_with_outliers.describe()\n\n\n\n\n\n\n\n\nidentifier\nlease\nlag_lease\nlag_market_value\nlag_dividend\nlag_loss\nlag_cash\nlag_tax_rate\nlag_big4_auditor\n\n\n\n\ncount\n6279.000000\n6279.000000\n6279.000000\n6279.000000\n6279.000000\n6279.000000\n6279.000000\n6279.000000\n6279.000000\n\n\nmean\n57841.428094\n1.951751\n1.778861\n7.625380\n0.016864\n0.236184\n0.196037\n0.255268\n0.821628\n\n\nstd\n66106.400906\n3.091940\n2.695251\n1.731492\n0.029755\n0.424770\n0.223529\n0.117834\n0.382856\n\n\nmin\n1004.000000\n0.000000\n0.032784\n3.607576\n0.000000\n0.000000\n0.000571\n0.000000\n0.000000\n\n\n25%\n10221.000000\n0.485310\n0.452138\n6.441416\n0.000000\n0.000000\n0.038528\n0.210000\n1.000000\n\n\n50%\n24171.000000\n0.919717\n0.845846\n7.535678\n0.000000\n0.000000\n0.109758\n0.338186\n1.000000\n\n\n75%\n116166.000000\n1.951153\n1.801071\n8.730227\n0.022831\n0.000000\n0.262504\n0.350000\n1.000000\n\n\nmax\n315318.000000\n34.634146\n15.867922\n12.190620\n0.203976\n1.000000\n0.960804\n0.380506\n1.000000\n\n\n\n\n\n\n\n\n# Show histograms - all variables except for the identifier\nsample_z = sample_with_outliers.drop(columns='identifier')\nsample_z.hist(bins = 20, figsize =(20, 10))\nplt.show()\n\n\n\n\n\n\n2.4. Drop variables without outliers\nThe above two panels show that lag_loss and lag_big4_auditor are indicator variables and thus do not have outliers.\nIn addition, the histogram suggests that lag_tax_rate also does not have outliers. During the sample period (2016-2019), the corporate tax rate was reduced from 35% to 21%. Two big spikes in the histogram around 35% and 21% and all other values smaller than 35% make economic sense.\n\n# Drop identifier, lag_loss, lag_big4_auditor, and lag_tax_rate \nvar = ['identifier', 'lag_loss', 'lag_big4_auditor', 'lag_tax_rate']\nsample_z = sample_with_outliers.drop(columns=var)\n\n\n\n\n3. Z-Score - Detect and Remove Outliers\nBefore implementing Isolation Forest, we will first attempt to detect outliers using the Z-score method.\nAs indicated in a previous section, the Z-Score method is effective in addressing outliers for data points that follow a normal distribution.\nI am going to assume that observations with a Z-score below -2.5 or above 2.5 (i.e., 2.5 standard deviations away from the mean; 1% of the sample) are outliers.\n\n3.1. lag_market_value - Identify and remove outliers\nThe histogram above shows that lag_market_value follows a normal distribution. To detect outliers, we first write a function to print the upper limits and lower limits of the Z-Score.\n\n# Create a function to report the limits of the Z-Score\ndef print_z_score_limits (df, column_name):\n    \"\"\" Print the upper and lower limits of the Z-score \"\"\"\n    \n    # Compute the limits\n    upper_limit = df[column_name].mean() + 3 * df[column_name].std()\n    lower_limit = df[column_name].mean() - 3 * df[column_name].std()\n    \n    # Round and return the limits\n    upper_limit = round(upper_limit, 2)\n    lower_limit = round(lower_limit, 2)\n    print_this = \"Variable Name: \" + column_name + \" | Upper limit: \" + str(upper_limit) + \" | Lower limit: \" + str(lower_limit)\n    return(print_this)\n\n\n# Print the upper and lower limits\nprint_z_score_limits(sample_z, \"lag_market_value\")\n\n'Variable Name: lag_market_value | Upper limit: 12.82 | Lower limit: 2.43'\n\n\nIt turns out that all of the values (N=6,279) are within the boundary values of 2.43 and 12.82. Thus, none of the observations are trimmed.\n\n# Filter outliers\nsample_z = sample_z[(sample_z['lag_market_value'] &gt;= 2.43) | (sample_z['lag_market_value'] &lt;= 12.82)]\nprint(sample_z.shape)\n\n(6279, 5)\n\n\nI’ll drop lag_market_value since an outlier treatment is not necessary for this feature.\n\n# Drop lag_market_value\nsample_z = sample_z.drop(columns=['lag_market_value'])\n\n\n\n3.2. Log transformation of other variables\nGoing back to the histogram above, we can see that lease, lag_lease, lag_dividend, and lag_cash are all significantly right-skewed. In this case, the Z-Score method or many other popular outlier detection methods such as the Interquartile Range (IQR) method won’t do any good. To address this issue, I conduct log transformations on these variables to see if we can describe them with normal distribution.\nThe histogram also shows that the four variables have many zeros (or very small values), which make economic sense. Therefore, I will only look for outliers on the right-hand side of the distribution.\nFirst, we will replace zeros with NaNs. This is okay because zeros will not be considered outliers.\n\n# Replace zeros with NaNs\nsample_z['lag_dividend'] = sample_z['lag_dividend'].replace([0],np.NaN)\nsample_z['lease'] = sample_z['lease'].replace([0],np.NaN)\n\nNext, I perform the log transformations and plot histograms\n\n# Create a function to conduct log transformation\ndef log_transformation_function (df, column_name):\n    \"\"\" Conduct a log transformation of a variable \"\"\"\n    # Replace the values with log-transformed values\n    df[[column_name]] = df[[column_name]].apply(np.log)\n\n# Conduct log transformation on all the variables\nfor column in sample_z:\n    log_transformation_function(sample_z, column)   \n    \n# Plot histograms\nsample_z.hist(bins = 20, figsize =(20, 10))\nplt.show()\n\n\n\n\n\n\n3.3. Other variables - Identify and remove outliers\nThe distributions now look much more like normal distributions. Once again, we will use the Z-Score to identify outliers.\nFirst, we report the Z-Score upper limits for each variable.\n\n# Print the upper and lower limits\nfor column in sample_z:\n    print(print_z_score_limits(sample_z, column))\n\nVariable Name: lease | Upper limit: 3.55 | Lower limit: -3.64\nVariable Name: lag_lease | Upper limit: 3.42 | Lower limit: -3.66\nVariable Name: lag_dividend | Upper limit: -0.8 | Lower limit: -6.86\nVariable Name: lag_cash | Upper limit: 1.97 | Lower limit: -6.76\n\n\nNext, we report the maximum values of each variable.\n\n# Print the maximum values\nprint(\"MAXIMUM VALUES\")\nprint(round(sample_z.max(),2))\n\nMAXIMUM VALUES\nlease           3.54\nlag_lease       2.76\nlag_dividend   -1.59\nlag_cash       -0.04\ndtype: float64\n\n\n\n\n\n4. Isolation Forest - Multi-dimensional Outlier Detection\nAlthough the data points do not seem to have outliers at the variable level, there could be outliers at a multi-dimensional level. Therefore, I employ Isolation Forest to detect outliers.\n\n4.1. Setup\nI begin by dropping the identifier from the original sample.\n\nsample_isf = sample_with_outliers.drop(columns='identifier')\n\n\n\n4.2. Conduct Principal Component Analysis (PCA)\nWe conduct PCA to reduce the firm feature dimensions from 7 to 2. Note that this step is not necessary because Isolation Forest works fine with multi-dimensions. Regardless, we reduce dimensions to visualize the outlier points in my data.\n\n# Standardize features\nsample_scaled = StandardScaler().fit_transform(sample_isf)\n\n# Define dimensions = 2\npca = PCA(n_components=2)       \n\n# Conduct the PCA\nprincipal_comp = pca.fit_transform(sample_scaled)     \n\n# Convert to dataframe\npca_df = pd.DataFrame(data = principal_comp, columns = ['principal_component_1', 'principal_component_2'])\npca_df.head()\n\n\n\n\n\n\n\n\nprincipal_component_1\nprincipal_component_2\n\n\n\n\n0\n-0.941123\n-0.078371\n\n\n1\n-0.908097\n0.062997\n\n\n2\n-0.984182\n-0.140106\n\n\n3\n-0.352054\n-0.265954\n\n\n4\n1.168329\n-0.220572\n\n\n\n\n\n\n\n\n\n4.3. Train the model and make predictions\nAs indicated before, we need to pre-define outlier frequency. After experimenting with data, we decide to use 4%.\n\n# Train the model\nisf = IsolationForest(contamination=0.04)\nisf.fit(pca_df)\n\n# Predictions\npredictions = isf.predict(pca_df)\n\n\n\n4.4. Extract predictions and isolation scores\n\n# Extract scores\npca_df[\"iso_forest_scores\"] = isf.decision_function(pca_df)\n\n# Extract predictions\npca_df[\"iso_forest_outliers\"] = predictions\n\n# Describe the dataframe\npca_df.describe()\n\n\n\n\n\n\n\n\nprincipal_component_1\nprincipal_component_2\niso_forest_scores\niso_forest_outliers\n\n\n\n\ncount\n6.279000e+03\n6.279000e+03\n6279.000000\n6279.000000\n\n\nmean\n-3.621177e-17\n1.810588e-17\n0.159245\n0.919732\n\n\nstd\n1.476402e+00\n1.413903e+00\n0.070999\n0.392577\n\n\nmin\n-3.451216e+00\n-1.375379e+00\n-0.139795\n-1.000000\n\n\n25%\n-1.047614e+00\n-7.105370e-01\n0.123628\n1.000000\n\n\n50%\n-3.918832e-01\n-4.407937e-01\n0.182017\n1.000000\n\n\n75%\n6.940018e-01\n9.532932e-02\n0.216277\n1.000000\n\n\nmax\n4.864363e+00\n1.051720e+01\n0.237358\n1.000000\n\n\n\n\n\n\n\nLet’s replace “-1” with “Yes” and “1” with “No”\n\n# Replace \"-1\" with \"Yes\" and \"1\" with \"No\"\npca_df['iso_forest_outliers'] = pca_df['iso_forest_outliers'].replace([-1, 1], [\"Yes\", \"No\"])\n\n# Print the first 5 firms\npca_df.head()\n\n\n\n\n\n\n\n\nprincipal_component_1\nprincipal_component_2\niso_forest_scores\niso_forest_outliers\n\n\n\n\n0\n-0.941123\n-0.078371\n0.200582\nNo\n\n\n1\n-0.908097\n0.062997\n0.188639\nNo\n\n\n2\n-0.984182\n-0.140106\n0.215439\nNo\n\n\n3\n-0.352054\n-0.265954\n0.225195\nNo\n\n\n4\n1.168329\n-0.220572\n0.174674\nNo\n\n\n\n\n\n\n\n\n\n\n4.5. Plots\nPlot the firms in the 2-dimensional space in the following order. [1] All firms [2] Normal Firms vs. Outlier Firms [3] Isolation Forest Scores\n\n# Create a function to plot firms on the 2-dimensional space\ndef plot_firms (dataframe, title, color = None):\n    \"\"\" Plot firms on the 2-dimensional space \"\"\"\n    \n    # Generate a scatter plot\n    fig = px.scatter(pca_df, x=\"principal_component_1\", y=\"principal_component_2\", title=title, color=color)\n    \n    # Layout\n    fig.update_layout(\n        font_family='Arial Black',\n        title=dict(font=dict(size=20, color='red')),\n        yaxis=dict(tickfont=dict(size=13, color='black'),\n                   titlefont=dict(size=15, color='black')),\n        xaxis=dict(tickfont=dict(size=13, color='black'),\n                   titlefont=dict(size=15, color='black')),\n        legend=dict(font=dict(size=10, color='black')),\n        plot_bgcolor='white',\n        # showlegend=False # Remove legend, if I want to\n    )\n    \n    ## Hide colorbar (run the following code if I want to)\n    #fig.update_coloraxes(showscale=False)\n    \n    return(fig)\n\n# Need to import renderers to view the plots on GitHub\nimport plotly.io as pio\n\n# Plot [1] All firms\nfig = plot_firms(pca_df, \"Figure 1: All Firms\")\nfig.show()\n\n\n                                                \n\n\n\n# [2] Normal Firms vs. Outlier Firms\nfig = plot_firms(dataframe=pca_df, title=\"Figure 2: Normal Firms vs. Outlier Firms\", color='iso_forest_outliers')\nfig.show()\n\n\n                                                \n\n\n\n# [3] Isolation Forest Scores\nfig = plot_firms(dataframe=pca_df, title=\"Figure 3: Isolation Forest Scores\", color='iso_forest_scores')\nfig.show()\n\n\n                                                \n\n\n\n4.6. Observations\nA few observations are in order.\nAccording to Figure 2, most of the outer points are identified as outliers. These outliers correctly meet the outlier characteristics that they are “distant and few.”\n\n\n\n5. Export and conclude\n\n# Add identifiers and cluster assignments (labels) to the sample\npca_df = pd.concat([sample_with_outliers['identifier'], pca_df], axis=1)\n\n# Print the first 5 firms\npca_df.head()\n\n\n\n\n\n\n\n\nidentifier\nprincipal_component_1\nprincipal_component_2\niso_forest_scores\niso_forest_outliers\n\n\n\n\n0\n1004\n-0.941123\n-0.078371\n0.200582\nNo\n\n\n1\n1004\n-0.908097\n0.062997\n0.188639\nNo\n\n\n2\n1004\n-0.984182\n-0.140106\n0.215439\nNo\n\n\n3\n1004\n-0.352054\n-0.265954\n0.225195\nNo\n\n\n4\n1050\n1.168329\n-0.220572\n0.174674\nNo\n\n\n\n\n\n\n\n\n# Export the sample as a csv file\npca_df.to_csv('outliers_detected.csv')"
  },
  {
    "objectID": "posts/anomaly/index.html#generating-random-variables-from-scratch",
    "href": "posts/anomaly/index.html#generating-random-variables-from-scratch",
    "title": "Random Variables",
    "section": "",
    "text": "This is a post with executable code.\nWe will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\nimport time\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\n\ndef pseudo_uniform_bad(mult=5,\n                       mod=11,\n                       seed=1,\n                       size=1):\n    \"\"\"\n    A bad pseudo random generator with small multipliers and modulus\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_bad(seed=3,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\ndef pseudo_uniform_good(mult=16807,\n                        mod=(2**31)-1,\n                        seed=123456789,\n                        size=1):\n    \"\"\"\n    A reasoanbly good pseudo random generator\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_good(size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\n\ndef sample_pick(lst):\n    \"\"\"\n    Picks up a random sample from a given list\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    # Random sample as an index\n    l = len(lst)\n    s = pseudo_uniform(low=0,high=l,seed=seed,size=1)\n    idx = int(s)\n    \n    return (lst[idx])\n\ndef pseudo_uniform(low=0,\n                   high=1,\n                  seed=123456789,\n                  size=1):\n    \"\"\"\n    Generates uniformly random number between `low` and `high` limits\n    \"\"\"\n    return low+(high-low)*pseudo_uniform_good(seed=seed,size=size)\n\ndice_faces = ['one','two','three','four','five','six']\n\n\nfor _ in range(30):\n    print(sample_pick(dice_faces),end=', ')\n\nthree, one, five, one, two, five, one, two, three, five, six, two, three, five, six, two, three, five, one, two, four, five, one, two, four, six, two, four, six, two, \n\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_37672\\1321908839.py:11: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n\n\n\n\nl = []\nfor _ in range(10000):\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    l.append(float(pseudo_uniform(0,6,seed=seed,size=1)))\n    \nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.xlim(-1,7)\nplt.show()\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_37672\\3436541096.py:5: DeprecationWarning:\n\nConversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n\n\n\n\n\n\n\n\n\n\ndef pseudo_bernoulli(p=0.5,size=1):\n    \"\"\"\n    Bernoulli generator from uniform generator\n    \"\"\"\n    # Sets seed based on the decimal portion of the current system clock\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    B = pseudo_uniform(seed=seed,size=size)\n    B = (B&lt;=p).astype(int)\n    \n    return B\n\nl=pseudo_bernoulli(p=0.2,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_binomial(n=100,\n                   p=0.5,\n                   size=1):\n    \"\"\"\n    Binomial distribution from the Uniform generator\n    \"\"\"\n    binom = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(size=n,seed=seed)\n        Y = (U &lt;= p).astype(int)\n        binom.append(np.sum(Y))\n    \n    return binom\n\n\n# 100 loaded coins, each with probability of head 0.75, are flipped \n# This trial/experiment is repeated for 15 times\n# The number of heads in each experiment are given below\npseudo_binomial(n=100,p=0.75,size=15)\n\n[73, 85, 71, 79, 72, 67, 67, 82, 70, 71, 75, 77, 80, 81, 87]\n\n\n\n\n\n\ndef pseudo_normal(mu=0.0,sigma=1.0,size=1):\n    \"\"\"\n    Generates Normal distribution from the Uniform distribution using Box-Muller transform\n    \"\"\"\n    # A pair of Uniform distributions\n    t = time.perf_counter()\n    seed1 = int(10**9*float(str(t-int(t))[0:]))\n    U1 = pseudo_uniform(seed=seed1,size=size)\n    t = time.perf_counter()\n    seed2 = int(10**9*float(str(t-int(t))[0:]))\n    U2 = pseudo_uniform(seed=seed2,size=size)\n    # Standard Normal pair\n    Z0 = np.sqrt(-2*np.log(U1))*np.cos(2*np.pi*U2)\n    Z1 = np.sqrt(-2*np.log(U1))*np.sin(2*np.pi*U2)\n    # Scaling\n    Z0 = Z0*sigma+mu\n    \n    return Z0\n\nl1=pseudo_normal(size=10000)\nplt.hist(l1,bins=25,edgecolor='k',alpha=0.5,color='blue')\nl2=pseudo_normal(mu=-3,sigma=2.0,size=10000)\nplt.hist(l2,bins=25,edgecolor='k',alpha=0.5,color='red')\nl3=pseudo_normal(mu=3,sigma=0.5,size=10000)\nplt.hist(l3,bins=25,edgecolor='k',alpha=0.5,color='green')\n\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend([\"$\\mu$:0, $\\sigma$:1.0\",\n           \"$\\mu$:-3, $\\sigma$:2.0\",\n           \"$\\mu$:3, $\\sigma$:0.5\"],fontsize=14)\nplt.show()\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n&lt;&gt;:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_37672\\1355573055.py:29: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_37672\\1355573055.py:30: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_37672\\1355573055.py:31: SyntaxWarning:\n\ninvalid escape sequence '\\m'\n\n\n\n\n\n\n\nl=pseudo_normal(size=10000)\nplt.hist(l,bins=25,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_exp(lamb,size=1):\n    \"\"\"\n    Generates exponential distribution from the Uniform distribution\n    \"\"\"\n    t = time.perf_counter()\n    seed = int(10**9*float(str(t-int(t))[0:]))\n    U = pseudo_uniform(size=size,seed=seed)\n    X = -(1/lamb)*(np.log(1-U))\n    \n    return X\n\nl=pseudo_exp(lamb=0.1,size=10000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n\n\n\n\n\n\n\ndef pseudo_poisson(alpha,size=1):\n    \"\"\"\n    \"\"\"\n    poisson = []\n    for _ in range(size):\n        t = time.perf_counter()\n        seed = int(10**9*float(str(t-int(t))[0:]))\n        U = pseudo_uniform(seed=seed,size=5*alpha)\n        X,P,i = 0,1,0\n        while P &gt;= np.exp(-alpha):\n            P = U[i]*P\n            X+=1\n            i+=1\n        poisson.append(X)\n    return np.array(poisson)\n\nl1=pseudo_poisson(alpha=5,size=10000)\nl2=pseudo_poisson(alpha=10,size=10000)\nl3=pseudo_poisson(alpha=20,size=10000)\n\nd1=dict(Counter(l1))\nd2=dict(Counter(l2))\nd3=dict(Counter(l3))\n\nk1 = [k for k in d1.keys()]\nv1 = [v for v in d1.values()]\nk2 = [k for k in d2.keys()]\nv2 = [v for v in d2.values()]\nk3 = [k for k in d3.keys()]\nv3 = [v for v in d3.values()]\n\n# Plotting\nplt.scatter(k1,v1,c='blue')\nplt.scatter(k2,v2,c='k')\nplt.scatter(k3,v3,c='green')\nplt.legend([\"Rate parameter \"+\"$\\lambda$: \"+str(i) for i in (5,10,20)],fontsize=15)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()\n\n&lt;&gt;:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'\n\n&lt;&gt;:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'\n\nC:\\Users\\Sabarish M N\\AppData\\Local\\Temp\\ipykernel_37672\\2970150594.py:36: SyntaxWarning:\n\ninvalid escape sequence '\\l'"
  }
]