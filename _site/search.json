[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myBlog",
    "section": "",
    "text": "Linear and Non Linear regression\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nK Means Clustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\n  \n\n\n\n\nK Means Clustering\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nSabarish Muthumani Narayanasamy\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Linear and Non Linear regression",
    "section": "",
    "text": "Blog 2 – Linear and Non Linear regression\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nImport packages\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nLoad the data\n\ndata = pd.read_csv(\"housing.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nlongitude\nlatitude\nhousing_median_age\ntotal_rooms\ntotal_bedrooms\npopulation\nhouseholds\nmedian_income\nmedian_house_value\nocean_proximity\n\n\n\n\n0\n-122.23\n37.88\n41.0\n880.0\n129.0\n322.0\n126.0\n8.3252\n452600.0\nNEAR BAY\n\n\n1\n-122.22\n37.86\n21.0\n7099.0\n1106.0\n2401.0\n1138.0\n8.3014\n358500.0\nNEAR BAY\n\n\n2\n-122.24\n37.85\n52.0\n1467.0\n190.0\n496.0\n177.0\n7.2574\n352100.0\nNEAR BAY\n\n\n3\n-122.25\n37.85\n52.0\n1274.0\n235.0\n558.0\n219.0\n5.6431\n341300.0\nNEAR BAY\n\n\n4\n-122.25\n37.85\n52.0\n1627.0\n280.0\n565.0\n259.0\n3.8462\n342200.0\nNEAR BAY\n\n\n\n\n\n\n\nThe info function provide the information about the dataset . For example:\n\nMissing values(no missing values in our dataset)\ndatatype(9 of them are floats and 1 is categorical)\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 10 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   longitude           20640 non-null  float64\n 1   latitude            20640 non-null  float64\n 2   housing_median_age  20640 non-null  float64\n 3   total_rooms         20640 non-null  float64\n 4   total_bedrooms      20433 non-null  float64\n 5   population          20640 non-null  float64\n 6   households          20640 non-null  float64\n 7   median_income       20640 non-null  float64\n 8   median_house_value  20640 non-null  float64\n 9   ocean_proximity     20640 non-null  object \ndtypes: float64(9), object(1)\nmemory usage: 1.6+ MB\n\n\nPearson correlation :\n\nplt.subplots(figsize=(15, 9))\ndata_numeric = data.select_dtypes(include=['float64', 'int64'])\ncor = data_numeric.corr()\nsns.heatmap(cor, annot=True, linewidths=.5)\nplt.show()\n\n\n\n\nIf we have to select a single variable for the regression analysis then higher possibility is to pick the most correlated feature with the target variable(median_house_value).\n\nIn our case it is the median_income with correlation coefficent of 0.69\n\n\n# taking two variables\ndata = data.drop([\"housing_median_age\",\"households\",\"total_bedrooms\",\"longitude\",\"latitude\",\"total_rooms\",\"population\",\"ocean_proximity\"], axis=1)\ndata.head()\n\n\n\n\n\n\n\n\nmedian_income\nmedian_house_value\n\n\n\n\n0\n8.3252\n452600.0\n\n\n1\n8.3014\n358500.0\n\n\n2\n7.2574\n352100.0\n\n\n3\n5.6431\n341300.0\n\n\n4\n3.8462\n342200.0\n\n\n\n\n\n\n\nUsing this scatter plot we can infer that if a person has higher median_income then that person may have more expensive house. There is somewhat positive linear relationship between them.\n\nX = data.drop(\"median_house_value\", axis=1)\ny = data[\"median_house_value\"]\nplt.scatter(X, y, alpha=0.5)\nplt.title('Scatter plot')\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\nplt.show()\n\n\n\n\n\nSplit the data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n\n\nModel 1:\n\nLinear regression model\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Model initialization\nregression_model = LinearRegression()\n\n# Fit the data(train the model)\nregression_model.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Predict\ny_predicted = regression_model.predict(X_test)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [42032.17769894]\nIntercept: 44320.6352276571\nRoot mean squared error:  84941.05152406936\nR2 score:  0.4466846804895944\n\n\nInterpretation:\nThis simple linear regression with single variable (y = mx+b) has\n\nSlope of the line(m) : [42032.17769894]\nIntercept (b) : 44320.63\nR2 score: 0.4466 (For R2 score more is better in the range [0,1])\nRoot mean squared error: 84941.0515 (Lower is better)\n\n#####The plot of simple linear regression :\n\n# data points\nplt.scatter(X_train, y_train, s=10)\nplt.xlabel('median_income')\nplt.ylabel('median_house_value')\n\n# predicted values\nplt.plot(X_test, y_predicted, color='r')\nplt.show()\n\n\n\n\n\n\n\n\nModel 2:\nFitting polynomial Regression model\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=2)\nX_poly = poly_reg.fit_transform(X_train)\npol_reg = LinearRegression()\npol_reg.fit(X_poly, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\ndef viz_polymonial():\n    plt.scatter(X_train, y_train, color=\"red\")\n    plt.plot(X_train, pol_reg.predict(poly_reg.fit_transform(X_train)))\n    plt.xlabel('median_income')\n    plt.ylabel('median_house_value')\n    plt.show()\n    return\nviz_polymonial()\n\n\n\n\n\n# Predict\nX_p = poly_reg.fit_transform(X_test)\ny_predicted = pol_reg.predict(X_p)\n\n# model evaluation\nrmse = np.sqrt(mean_squared_error(y_test, y_predicted))\nr2 = r2_score(y_test, y_predicted)\n\n# printing values\nprint('Slope:' ,regression_model.coef_)\nprint('Intercept:', regression_model.intercept_)\nprint('Root mean squared error: ', rmse)\nprint('R2 score: ', r2)\n\nSlope: [42032.17769894]\nIntercept: 44320.6352276571\nRoot mean squared error:  84699.90676455045\nR2 score:  0.44982190770645947\n\n\nInterpretation:\nThis transformed linear regression with single variable (y = mx+b) has\n\nSlope of the line(m) : 175550.81\nIntercept (b) : -129097.46\nR2 score: 0.4498 (For R2 score more is better in the range [0,1])\nFound R2 score is the best so far. This means that we will keep this ploynomial model with degree 2 as our final and best model(but there is one other thing to consider i.e. simple is better than complex)\n\nRoot mean squared error: 84699.9 (Lower is better)\n\n\nComparing the Model\n\nModel 1 has R2 score: 0.4466\nModel 2 has R2 score: 0.44982 After analyzing the R2 score , My final model will be Model 1 as it is simple and has not worse R2 score as compared to the model 3."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "K Means Clustering",
    "section": "",
    "text": "Blog 1 – Clustering\nThe aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc.\nThis is a post with executable code.\nWe will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\nNext, let’s read the data and look at the first five rows:\n\ndata=pd.read_csv(\"customer_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nChannel\nRegion\nFresh\nMilk\nGrocery\nFrozen\nDetergents_Paper\nDelicassen\n\n\n\n\n0\n2\n3\n12669\n9656\n7561\n214\n2674\n1338\n\n\n1\n2\n3\n7057\n9810\n9568\n1762\n3293\n1776\n\n\n2\n2\n3\n6353\n8808\n7684\n2405\n3516\n7844\n\n\n3\n1\n3\n13265\n1196\n4221\n6404\n507\n1788\n\n\n4\n2\n3\n22615\n5410\n7198\n3915\n1777\n5185\n\n\n\n\n\n\n\nHere, we see that there is a lot of variation in the magnitude of the data. Variables like Channel and Region have low magnitude, whereas variables like Fresh, Milk, Grocery, etc., have a higher magnitude.\nSince K-Means is a distance-based algorithm, this difference in magnitude can create a problem. Bring all the variables to the same magnitude\n\n# standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# statistics of scaled data\npd.DataFrame(data_scaled).describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\ncount\n4.400000e+02\n4.400000e+02\n4.400000e+02\n440.000000\n4.400000e+02\n4.400000e+02\n4.400000e+02\n4.400000e+02\n\n\nmean\n1.614870e-17\n3.552714e-16\n-3.431598e-17\n0.000000\n-4.037175e-17\n3.633457e-17\n2.422305e-17\n-8.074349e-18\n\n\nstd\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138e+00\n\n\nmin\n-6.902971e-01\n-1.995342e+00\n-9.496831e-01\n-0.778795\n-8.373344e-01\n-6.283430e-01\n-6.044165e-01\n-5.402644e-01\n\n\n25%\n-6.902971e-01\n-7.023369e-01\n-7.023339e-01\n-0.578306\n-6.108364e-01\n-4.804306e-01\n-5.511349e-01\n-3.964005e-01\n\n\n50%\n-6.902971e-01\n5.906683e-01\n-2.767602e-01\n-0.294258\n-3.366684e-01\n-3.188045e-01\n-4.336004e-01\n-1.985766e-01\n\n\n75%\n1.448652e+00\n5.906683e-01\n3.905226e-01\n0.189092\n2.849105e-01\n9.946441e-02\n2.184822e-01\n1.048598e-01\n\n\nmax\n1.448652e+00\n5.906683e-01\n7.927738e+00\n9.183650\n8.936528e+00\n1.191900e+01\n7.967672e+00\n1.647845e+01\n\n\n\n\n\n\n\nCreate a kmeans function and fit it on the data\n\n# defining the kmeans function with initialization as k-means++\nkmeans = KMeans(n_clusters=2, init='k-means++')\n\n# fitting the k means algorithm on scaled data\nkmeans.fit(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nKMeans(n_clusters=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=2)\n\n\nLet’s evaluate how well the formed clusters are. To do that, we will calculate the inertia of the clusters:\n\n# inertia on the fitted data\nkmeans.inertia_\n\n2599.3844237836256\n\n\nWe will store the inertia value of each model and then plot it to visualize the result\n\nSSE = []\nfor cluster in range(1,20):\n    kmeans = KMeans(n_clusters = cluster, init='k-means++')\n    kmeans.fit(data_scaled)\n    SSE.append(kmeans.inertia_)\n\n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nText(0, 0.5, 'Inertia')\n\n\n\n\n\nLooking at the above elbow curve, we can choose any number of clusters between 5 to 8.\nSet the number of clusters as 6 and fit the model.\n\n# k means using 5 clusters and k-means++ initialization\nkmeans = KMeans(n_clusters = 5, init='k-means++')\nkmeans.fit(data_scaled)\npred = kmeans.predict(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nValue count of points in each of the above-formed clusters\n\nframe = pd.DataFrame(data_scaled)\nframe['cluster'] = pred\nframe['cluster'].value_counts()\n\ncluster\n3    200\n4    126\n0     90\n2     14\n1     10\nName: count, dtype: int64\n\n\nSo, there are 234 data points belonging to cluster 4 (index 3), 125 points in cluster 2 (index 1), and so on.\nNow, lets plot the clusters according to midpoints and differentiate based on colours.\n\nplt.scatter(data_scaled[:, 2], data_scaled[:, 3], c=pred, s=50, cmap='viridis')\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 2], centers[:, 4], c='black', s=200, alpha=0.5);"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html#generating-random-variables-from-scratch",
    "href": "posts/welcome/index.html#generating-random-variables-from-scratch",
    "title": "Linear and Non Linear regression",
    "section": "Generating random variables from scratch",
    "text": "Generating random variables from scratch\nRandom variables are a fundamental component of data science, machine learning, and statistical modeling. They play a growingly significant role in the systems and services that utilize artificial intelligence or deep neural networks.\nThe idea and characteristics of random variables are applied in various areas such as\n\npredictive modeling methods,\ncollaborative techniques like stochastic forests, gradient boosting,\nneural network architectures,\ngrouping algorithms,\nnatural language processing,\nreinforcement learning,\nsophisticated search algorithms in AI and game theory.\n\nThis document aims to illustrate how one can create random variables from the ground up through basic programming.\n\nThe code and the demo\n\nUniform random generator based on “Linear Congruential Generator”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport time\n\n\ndef pseudo_uniform_bad(mult=5,\n                       mod=11,\n                       seed=1,\n                       size=1):\n    \"\"\"\n    A bad pseudo random generator with small multipliers and modulus\n    \"\"\"\n    U = np.zeros(size)\n    x = (seed*mult+1)%mod\n    U[0] = x/mod\n    for i in range(1,size):\n        x = (x*mult+1)%mod\n        U[i] = x/mod\n    return U\n\n\nl=pseudo_uniform_bad(seed=3,size=1000)\nplt.hist(l,bins=20,edgecolor='k')\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code copy/index.html",
    "href": "posts/post-with-code copy/index.html",
    "title": "K Means Clustering",
    "section": "",
    "text": "Blog 1 – Clustering\nThe aim of this problem is to segment the clients of a wholesale distributor based on their annual spending on diverse product categories, like milk, grocery, region, etc.\nThis is a post with executable code.\nWe will first import the required libraries:\n\n# importing required libraries\nimport logging\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.cluster import KMeans\n\nlogging.getLogger('sklearn').setLevel(logging.WARNING)\n\nNext, let’s read the data and look at the first five rows:\n\ndata=pd.read_csv(\"customer_data.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nChannel\nRegion\nFresh\nMilk\nGrocery\nFrozen\nDetergents_Paper\nDelicassen\n\n\n\n\n0\n2\n3\n12669\n9656\n7561\n214\n2674\n1338\n\n\n1\n2\n3\n7057\n9810\n9568\n1762\n3293\n1776\n\n\n2\n2\n3\n6353\n8808\n7684\n2405\n3516\n7844\n\n\n3\n1\n3\n13265\n1196\n4221\n6404\n507\n1788\n\n\n4\n2\n3\n22615\n5410\n7198\n3915\n1777\n5185\n\n\n\n\n\n\n\nHere, we see that there is a lot of variation in the magnitude of the data. Variables like Channel and Region have low magnitude, whereas variables like Fresh, Milk, Grocery, etc., have a higher magnitude.\nSince K-Means is a distance-based algorithm, this difference in magnitude can create a problem. Bring all the variables to the same magnitude\n\n# standardizing the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndata_scaled = scaler.fit_transform(data)\n\n# statistics of scaled data\npd.DataFrame(data_scaled).describe()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n\n\n\n\ncount\n4.400000e+02\n4.400000e+02\n4.400000e+02\n440.000000\n4.400000e+02\n4.400000e+02\n4.400000e+02\n4.400000e+02\n\n\nmean\n1.614870e-17\n3.552714e-16\n-3.431598e-17\n0.000000\n-4.037175e-17\n3.633457e-17\n2.422305e-17\n-8.074349e-18\n\n\nstd\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138\n1.001138e+00\n1.001138e+00\n1.001138e+00\n1.001138e+00\n\n\nmin\n-6.902971e-01\n-1.995342e+00\n-9.496831e-01\n-0.778795\n-8.373344e-01\n-6.283430e-01\n-6.044165e-01\n-5.402644e-01\n\n\n25%\n-6.902971e-01\n-7.023369e-01\n-7.023339e-01\n-0.578306\n-6.108364e-01\n-4.804306e-01\n-5.511349e-01\n-3.964005e-01\n\n\n50%\n-6.902971e-01\n5.906683e-01\n-2.767602e-01\n-0.294258\n-3.366684e-01\n-3.188045e-01\n-4.336004e-01\n-1.985766e-01\n\n\n75%\n1.448652e+00\n5.906683e-01\n3.905226e-01\n0.189092\n2.849105e-01\n9.946441e-02\n2.184822e-01\n1.048598e-01\n\n\nmax\n1.448652e+00\n5.906683e-01\n7.927738e+00\n9.183650\n8.936528e+00\n1.191900e+01\n7.967672e+00\n1.647845e+01\n\n\n\n\n\n\n\nCreate a kmeans function and fit it on the data\n\n# defining the kmeans function with initialization as k-means++\nkmeans = KMeans(n_clusters=2, init='k-means++')\n\n# fitting the k means algorithm on scaled data\nkmeans.fit(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nKMeans(n_clusters=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=2)\n\n\nLet’s evaluate how well the formed clusters are. To do that, we will calculate the inertia of the clusters:\n\n# inertia on the fitted data\nkmeans.inertia_\n\n2599.3873849123083\n\n\nWe will store the inertia value of each model and then plot it to visualize the result\n\nSSE = []\nfor cluster in range(1,20):\n    kmeans = KMeans(n_clusters = cluster, init='k-means++')\n    kmeans.fit(data_scaled)\n    SSE.append(kmeans.inertia_)\n\n# converting the results into a dataframe and plotting them\nframe = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\nplt.figure(figsize=(12,6))\nplt.plot(frame['Cluster'], frame['SSE'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nText(0, 0.5, 'Inertia')\n\n\n\n\n\nLooking at the above elbow curve, we can choose any number of clusters between 5 to 8.\nSet the number of clusters as 6 and fit the model.\n\n# k means using 5 clusters and k-means++ initialization\nkmeans = KMeans(n_clusters = 5, init='k-means++')\nkmeans.fit(data_scaled)\npred = kmeans.predict(data_scaled)\n\nC:\\Users\\Sabarish M N\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\nValue count of points in each of the above-formed clusters\n\nframe = pd.DataFrame(data_scaled)\nframe['cluster'] = pred\nframe['cluster'].value_counts()\n\ncluster\n0    210\n4    126\n1     91\n2     12\n3      1\nName: count, dtype: int64\n\n\nSo, there are 234 data points belonging to cluster 4 (index 3), 125 points in cluster 2 (index 1), and so on.\nNow, lets plot the clusters according to midpoints and differentiate based on colours.\n\nplt.scatter(data_scaled[:, 2], data_scaled[:, 3], c=pred, s=50, cmap='viridis')\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 2], centers[:, 4], c='black', s=200, alpha=0.5);"
  }
]